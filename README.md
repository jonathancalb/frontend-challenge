# FL105 Frontend Take-Home Challenge
![Figma Design Preview](./design.png)

## Overview
This repository contains a take-home coding challenge focused on frontend development. The task is to build an interactive chat interface based on a provided Figma design. The challenge is designed to assess your ability to deliver a polished, responsive, and accessible UI with engaging micro-interactions.

## Challenge Summary
- **Goal:** Implement a chat interface from the provided Figma design, with streaming message appearance and a typing indicator.
- **Tech Stack:** React (any framework), TypeScript preferred. Styling approach is up to you (CSS Modules, Tailwind, styled-components, etc.).
- **Time Limit:** Please spend no more than 2–3 hours. It’s okay to leave TODOs.

## Core Requirements
1. **High-fidelity design:** Match the Figma design closely (spacing, typography, states).
2. **Visual distinction:** Clearly show when the user or AI assistant is speaking.
3. **Mood/tone indicators:** Creatively interpret and display conversation mood/tone changes.
4. **Responsive design:** Works well on desktop and mobile.
5. **Accessibility:** Keyboard navigation, ARIA labels, visible focus states.
6. **Streaming messages:** Assistant messages should appear character-by-character or chunk-by-chunk.
7. **Typing indicator:** Animated dots or waveform pulse while streaming.
8. **Message list behavior:** Auto-scroll while streaming.
9. **Error state:** Show a user-friendly error if sending fails.

## Nice-to-Have (Optional)
- Message retry/cancel.
- Light/dark theme toggle.

## Deliverables
- A working prototype (hosted or runnable locally).
- This README, documenting:
	- What you accomplished
	- Any assumptions made
	- Trade-offs and how you would continue the task with more time (plus a time estimate)

## Figma Design
[View the Figma design](https://www.figma.com/design/AL0hlU5r67DCykEUBRe3yH/FE-Code-Challenge)
password: Iris

## Getting Started
1. Clone this repo.
2. Install dependencies (`yarn`).
3. Start the development server (`yarn run dev`).

**TL;DR**: To test error handling, uncomment line 54 in `src/mocks/api/aiService.ts` and try sending a message in Iris Text mode.

## What I Accomplished

- **Mobile-First Design**: Implemented a mobile wrapper component that simulates a phone interface in development mode, matching the Figma design specifications with proper device dimensions and styling
- **Complete Navigation System**: Built a comprehensive bottom navigation menu with all four sections (Iris, Chat, Discover, Insights) featuring active state highlighting and smooth transitions
- **Iris Talk & Text Functionality**: Implemented both voice recording (Talk) and text chat (Text) modes with full conversation flow, including message streaming effects and loading states
- **API Mocking & Demo Ready**: Created a complete mock API system for demonstration purposes, allowing the app to function fully without backend dependencies
- **Atomic Design Architecture**: Built a comprehensive component library following atomic design principles with atoms, molecules, organisms, and templates for maintainable, scalable code
- **Separation of Concerns**: Organized the codebase with proper separation between UI components, business logic (hooks), state management, and API layers as appropriate for a small application
- **Accessibility & Rich Interactions**: Implemented comprehensive accessibility features including ARIA labels, keyboard navigation, screen reader support, and smooth animations throughout the interface
- **Error Handling**: Built a complete error state system with user-friendly error messages, retry functionality, and dismiss options that can be easily tested by uncommenting a single line

## Assumptions Made

### Development Approach
**Reality Check**: While I wanted to dedicate the suggested 2-3 hours to this challenge, I ended up needing almost **2 days** to deliver this MVP with the level of polish and attention to detail shown here. This included implementing atomic design patterns, proper component architecture, responsive considerations, and all the micro-interactions.

Given the suggested 2-3 hour timeframe, it was clear that leveraging AI assistance was not just helpful but expected—without it, delivering this level of functionality and polish would be improbable.

**AI-Assisted Development Approach:**
While I used AI tools for rapid prototyping, it's important to note that I **guided all architectural decisions** and provided explicit direction for the component structure, state management patterns, and design implementation. I **thoroughly reviewed every piece of code** generated by the AI tools and made all the fine-tuning adjustments myself. The AI served as an accelerated coding partner, but the technical decisions, code quality standards, and architectural choices were entirely my responsibility.

**Tools Used:**
- **Cursor with Claude-4-Sonnet**: Rapid prototyping
- **Tailwind CSS**: Fast styling and mobile-first responsive utilities
- **React + TypeScript**: Type-safe component development
- **ESLint**: Code quality and unused import detection

### Key Design Assumptions
- **Mobile-only scope**: Based on the Figma design showing a mobile interface (iPhone-style status bar, mobile proportions), I implemented this as a mobile-first, mobile-only experience. Optimized for mobile devices with touch interactions, fixed max-width container (~448px), touch-friendly button sizes, mobile status bar simulation, and no hover states (using `active:` states instead)
- **Chat UX Pattern**: Implemented ChatGPT-like behavior where users can type during AI responses but cannot send until the response completes. This prevents message queue issues and provides clear feedback. Since this pattern wasn't specified in the design, I researched modern AI chat interfaces and adopted this industry-standard approach for better UX.
- **Design Tokens Implementation**: I assumed we would use a design token system (following atomic design principles) and explored where tokens could be applied throughout the codebase. I implemented a foundational token system (`/src/tokens/design-tokens.css`) with colors, spacing, typography, and component tokens. However, this is something that would need to be agreed upon and refined with the design team to ensure alignment with their design system and naming conventions.
- **Navigation Animation**: Implemented slide-down animation for bottom navigation when switching between Iris modes (Talk/Text) - this would normally require discussion with the design team to ensure it aligns with intended user experience and accessibility requirements
- **Voice-to-Text Continuity**: It's not completely clear if the conversation you have on voice mode should continue in chat mode as transcribed text. Probably that would be the best experience, but it's difficult to simulate something like that in this mock

## Trade-offs Made
1. **Mobile-Only Focus**: Prioritized perfect mobile experience over responsive design
2. **Simulated Functionality**: UI states and animations over actual voice recording
3. **Design Precision**: Didn't have time to ensure pixel-perfect alignment with Figma - would invest more time in this
4. **Simple Animations**: CSS-based animations that could be improved with more time
5. **Mock Design Elements**: Included hardcoded status bar elements (time, battery, etc.) for design fidelity
6. **Design Token Limitations**: Atomic design could be better if tokenization came directly from Figma rather than manual implementation
7. **State Management**: Used `useState` for local component state management due to time constraints. With more time, would evaluate migrating to more robust solutions like `useReducer`, Context + Reducer, or external libraries like Redux Toolkit, Zustand, or Jotai for larger-scale state management
8. **Navigation Features**: Decided not to implement the subtle glow/halo effect around active navigation items from the Figma design to prioritize core functionality within the time constraint
9. **Static Mood Colors**: Due to time constraints, didn't implement the dynamic mood progression feature where the orbs and glowing effects change colors as the conversation mood advances

## What I Would Improve With More Time

- **Design Collaboration**: Talk with the design team to understand their exact vision for animations, micro-interactions, and overall user experience
- **Pixel-Perfect Implementation**: Ensure everything matches the design specifications exactly
- **Responsive Design**: Adapt to desktop, tablets, and other screen sizes - would require the design team to provide those design decisions and specifications
- **Scope Definition**: Define the application scope regarding target devices, audience, and markets to inform technical decisions
- **State Management**: Evaluate if it's worth using a state management library based on application complexity
- **Real API Integration**: Connect to actual chat and audio APIs instead of mocks
- **Testing**: Implement unit testing and e2e testing (probably in a separate repo)
- **Mood System**: Implement the dynamic mood progression feature where colors evolve with conversation mood
- **Chat Summaries**: Add conversation context and participant summaries functionality
- **Missing Sections**: Implement the remaining sections (Chat, Discover, Insights) with full functionality
- **Production Monitoring**: Integrate tools like Sentry for error tracking and monitoring
- **Deployment Strategy**: Define deployment processes, CI/CD pipelines, and workflow
- **Animation Improvements**: Enhance animations and micro-interactions based on design team feedback

This MVP demonstrates core interaction patterns and visual design, but production deployment would require addressing these broader strategic and technical considerations.

**Note on Time Estimates**: I can't provide specific time estimates for improvements because many depend on the scope of the application regarding functionality, target markets, devices, and other requirements that would need to be defined.